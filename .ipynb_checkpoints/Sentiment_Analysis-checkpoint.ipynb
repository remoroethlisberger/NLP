{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.stem import PorterStemmer # we won't be doing stemming and rather perform lemmatizing\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# the sentiments we want to analyze\n",
    "sentiments = ['pos', 'neg']\n",
    "# the raw data we will store in an object for now\n",
    "data = {}\n",
    "\n",
    "# set of stopwords, we want to remove\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# our lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "for i, sentiment in enumerate(sentiments):\n",
    "    files = os.listdir('./movie_review/txt_sentoken/' + sentiment)\n",
    "    data[sentiment] = []\n",
    "    for i, filename in enumerate(files):\n",
    "        data[sentiment].append([])\n",
    "        with open('./movie_review/txt_sentoken/' + sentiment + '/' + filename) as f:\n",
    "            review = f.read()\n",
    "            words = word_tokenize(review)\n",
    "\n",
    "            for word in words:\n",
    "                if word not in stop_words and word.isalpha():\n",
    "                    token = str(lemmatizer.lemmatize(word))\n",
    "                    data[sentiment][i].append(token)\n",
    "\n",
    "# -> cleaned data - removed stop words and punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the positive and negative vocab\n",
    "with open('./opinion-lexicon/negative-words.txt') as f:\n",
    "    negative_words = f.read()\n",
    "    negative_words = negative_words.split(\"\\n\")\n",
    "    neg_vocab = negative_words[:-1] # we do not want the last empty line\n",
    "    \n",
    "with open('./opinion-lexicon/positive-words.txt') as f:\n",
    "    positive_words = f.read()\n",
    "    positive_words = positive_words.split(\"\\n\")\n",
    "    pos_vocab = positive_words[:-1] # we do not want the last empty line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the bag of words\n",
    "statistics = {}\n",
    "for i, sentiment in enumerate(sentiments):\n",
    "    statistics[sentiment] = []\n",
    "    for j, review in enumerate(data[sentiment]):\n",
    "        bag_pos = np.zeros(len(pos_vocab))\n",
    "        bag_neg = np.zeros(len(neg_vocab))\n",
    "        for k, word in enumerate(data[sentiment][j]):\n",
    "            for l, vocab in enumerate(neg_vocab):\n",
    "                if word == vocab:\n",
    "                    bag_neg[l] += 1\n",
    "            for m, vocab in enumerate(pos_vocab):\n",
    "                if word == vocab:\n",
    "                    bag_pos[m] += 1\n",
    "        \n",
    "        statistics[sentiment].append({\n",
    "            'bag_of_words_positive': bag_pos,\n",
    "            'bag_of_words_negative': bag_neg\n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_movie(bag_pos, bag_neg, actual):\n",
    "    positive_rate = np.sum(bag_pos)\n",
    "    negative_rate = np.sum(bag_neg)\n",
    "\n",
    "    if positive_rate >= negative_rate:\n",
    "        correct = 1 if 'pos' == actual else 0\n",
    "        guess = 1\n",
    "        target = 1 if 'pos' == actual else -1\n",
    "        return np.concatenate((bag_pos, bag_neg, np.array([target, guess, correct])))\n",
    "    elif positive_rate < negative_rate:\n",
    "        correct = 1 if 'neg' == actual else 0\n",
    "        guess = -1\n",
    "        target = 1 if 'pos' == actual else -1\n",
    "        return np.concatenate((bag_pos, bag_neg, np.array([target, guess, correct])))\n",
    "\n",
    "    raise Exception(\"Ui, error!\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = pd.DataFrame()\n",
    "correct_column = len(pos_vocab) + len(neg_vocab) + 2\n",
    "target_column = len(pos_vocab) + len(neg_vocab)\n",
    "for i, sentiment in enumerate(sentiments):\n",
    "    for j, review in enumerate(statistics[sentiment]):\n",
    "        v = classify_movie(statistics[sentiment][j]['bag_of_words_positive'], statistics[sentiment][j]['bag_of_words_negative'], sentiment)\n",
    "        d = d.append(pd.DataFrame(v.reshape(-1, len(v))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7015\n"
     ]
    }
   ],
   "source": [
    "correct_guesses = d[d[correct_column] == 1].shape[0]\n",
    "print(\"Accuracy: %.4f\" % (correct_guesses/d.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.7251\n"
     ]
    }
   ],
   "source": [
    "true_positives = d[(d[correct_column] == 1) & (d[target_column] == 1)].shape[0]\n",
    "false_positive = d[(d[correct_column] == 0) & (d[target_column] == -1)].shape[0]\n",
    "print(\"Precision: %.4f\" % (true_positives/(true_positives + false_positive)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall: 0.6490\n",
      "F-Measure: 0.5229\n"
     ]
    }
   ],
   "source": [
    "true_positives = d[(d[correct_column] == 1) & (d[target_column] == 1)].shape[0]\n",
    "false_negatives = d[(d[correct_column] == 0) & (d[target_column] == 1)].shape[0]\n",
    "print(\"Recall: %.4f\" % (true_positives/(true_positives + false_negatives)))\n",
    "print(\"F-Measure: %.4f\" % (2. / (1./(true_positive/(true_positive + false_positive)) + 1. /(true_positive/(true_positive + false_negative)))) ) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discriminative Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "shuffled_d = d.sample(frac=1)\n",
    "test_bench = shuffled_d.iloc[:400]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_bench = shuffled_d.iloc[400:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = training_bench.filter(items=[target_column])\n",
    "X = training_bench.drop(d.columns[-3:],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "discriminative_clf = LogisticRegression().fit(X,y[target_column])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1., -1., -1., -1., -1.,  1., -1., -1.,  1.,  1., -1.,  1., -1.,\n",
       "        1., -1.,  1.,  1., -1.,  1.,  1., -1.,  1., -1.,  1.,  1., -1.,\n",
       "       -1., -1., -1.,  1.,  1., -1.,  1.,  1.,  1., -1.,  1., -1.,  1.,\n",
       "        1., -1.,  1.,  1.,  1.,  1., -1., -1., -1., -1.,  1., -1., -1.,\n",
       "       -1., -1., -1., -1.,  1.,  1., -1., -1.,  1.,  1., -1., -1.,  1.,\n",
       "       -1.,  1., -1.,  1., -1.,  1., -1., -1.,  1., -1.,  1., -1.,  1.,\n",
       "        1.,  1., -1.,  1.,  1.,  1., -1.,  1.,  1., -1.,  1.,  1., -1.,\n",
       "       -1.,  1., -1., -1., -1.,  1., -1.,  1., -1., -1.,  1., -1.,  1.,\n",
       "       -1., -1.,  1., -1., -1., -1.,  1., -1.,  1., -1.,  1., -1., -1.,\n",
       "       -1., -1.,  1., -1., -1., -1., -1.,  1.,  1.,  1.,  1.,  1., -1.,\n",
       "       -1.,  1., -1.,  1., -1., -1.,  1., -1., -1.,  1.,  1.,  1.,  1.,\n",
       "       -1.,  1.,  1., -1., -1., -1.,  1.,  1., -1., -1.,  1.,  1.,  1.,\n",
       "        1.,  1.,  1.,  1.,  1.,  1.,  1., -1.,  1.,  1., -1., -1.,  1.,\n",
       "        1.,  1.,  1.,  1., -1., -1., -1., -1.,  1., -1.,  1., -1., -1.,\n",
       "       -1., -1.,  1.,  1.,  1., -1.,  1.,  1., -1.,  1., -1.,  1.,  1.,\n",
       "        1.,  1., -1.,  1., -1., -1.,  1., -1., -1., -1.,  1., -1.,  1.,\n",
       "       -1.,  1.,  1.,  1., -1., -1., -1., -1.,  1., -1., -1., -1.,  1.,\n",
       "       -1., -1., -1.,  1.,  1., -1., -1., -1., -1.,  1., -1., -1., -1.,\n",
       "       -1., -1.,  1., -1.,  1., -1.,  1., -1.,  1.,  1., -1.,  1.,  1.,\n",
       "        1.,  1., -1.,  1.,  1., -1.,  1., -1.,  1., -1., -1., -1.,  1.,\n",
       "        1., -1.,  1.,  1., -1.,  1., -1., -1.,  1., -1., -1., -1.,  1.,\n",
       "       -1., -1., -1., -1.,  1., -1., -1., -1., -1., -1., -1., -1.,  1.,\n",
       "        1.,  1.,  1., -1., -1., -1., -1., -1.,  1.,  1., -1., -1., -1.,\n",
       "        1., -1.,  1.,  1., -1., -1., -1.,  1., -1.,  1., -1., -1.,  1.,\n",
       "        1., -1.,  1.,  1., -1.,  1., -1., -1.,  1.,  1., -1.,  1., -1.,\n",
       "       -1., -1., -1., -1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1., -1.,\n",
       "       -1., -1.,  1., -1., -1., -1., -1.,  1., -1., -1., -1., -1.,  1.,\n",
       "        1., -1., -1., -1.,  1.,  1., -1., -1.,  1., -1., -1.,  1., -1.,\n",
       "        1., -1.,  1., -1., -1., -1., -1.,  1., -1., -1.,  1.,  1.,  1.,\n",
       "       -1.,  1., -1.,  1.,  1.,  1.,  1., -1., -1., -1.,  1.,  1., -1.,\n",
       "        1.,  1.,  1., -1.,  1.,  1., -1., -1.,  1.,  1.])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_prime = test_bench.drop(d.columns[-3:],axis=1)\n",
    "y_prime = test_bench.filter(items=[target_column])\n",
    "discriminative_clf.predict(X_prime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.86"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "discriminative_clf.score(X_prime, y_prime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_prime = y_prime.reset_index().filter(items=[target_column])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = pd.DataFrame(discriminative_clf.predict(X_prime)).join(y_prime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_positive = s[(s[0] == s[target_column]) & (s[target_column] == 1)].shape[0]\n",
    "true_negative = s[(s[0] == s[target_column]) & (s[target_column] == -1)].shape[0]\n",
    "false_positive = s[~(s[0] == s[target_column]) & (s[target_column] == -1)].shape[0]\n",
    "false_negative = s[~(s[0] == s[target_column]) & (s[target_column] == 1)].shape[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy 0.8600\n",
      "Precision: 0.8571\n",
      "Recall: 0.8482\n",
      "F-Measure: 0.8526\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy %.4f\" % accuracy_score(discriminative_clf.predict(X_prime), y_prime[target_column]))\n",
    "print(\"Precision: %.4f\" % (true_positive/(true_positive + false_positive)))\n",
    "print(\"Recall: %.4f\" % (true_positive/(true_positive + false_negative)))\n",
    "print(\"F-Measure: %.4f\" % (2. / (1./(true_positive/(true_positive + false_positive)) + 1. /(true_positive/(true_positive + false_negative)))) ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py36",
   "language": "python",
   "name": "py36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
